{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ofornes/Work/GRECO/JASPAR-profile-inference',\n",
       " '/home/ofornes/Work/GRECO',\n",
       " '/home/ofornes/Work/Anaconda3/envs/JASPAR-profile-inference/lib/python37.zip',\n",
       " '/home/ofornes/Work/Anaconda3/envs/JASPAR-profile-inference/lib/python3.7',\n",
       " '/home/ofornes/Work/Anaconda3/envs/JASPAR-profile-inference/lib/python3.7/lib-dynload',\n",
       " '',\n",
       " '/home/ofornes/Work/Anaconda3/envs/JASPAR-profile-inference/lib/python3.7/site-packages',\n",
       " '/home/ofornes/Work/Anaconda3/envs/JASPAR-profile-inference/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/home/ofornes/.ipython']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append JASPAR-profile-inference to path\n",
    "jaspar_dir = os.path.abspath(\"./JASPAR-profile-inference\")\n",
    "sys.path.insert(0, jaspar_dir)\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from JASPAR-profile-inference\n",
    "from __init__ import Jglobals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------#\n",
    "# Class       #\n",
    "#-------------#\n",
    "\n",
    "class TF(object):\n",
    "\n",
    "    def __init__(self, gene_name, species):\n",
    "\n",
    "        self.gene_name = gene_name\n",
    "        self.species = species\n",
    "        self.uniacc = None\n",
    "        self.unientry = None\n",
    "        self.status = None\n",
    "        self.sequence = None\n",
    "        self.family = \"Unknown\"\n",
    "        self.cluster_num = None\n",
    "        # self.orthodb = set()\n",
    "        self.jaspar_id = None\n",
    "        self.hocomoco_id = set()\n",
    "        \n",
    "        # In vivo\n",
    "        self.chip_atlas = set()\n",
    "        self.cistromedb = set()\n",
    "        self.gtrd = set()\n",
    "        self.dap_seq = set()\n",
    "        self.remap = set()\n",
    "\n",
    "        # In vitro\n",
    "        self.ht_selex = set()\n",
    "        self.cisbp = set()\n",
    "        self.uniprobe = set()\n",
    "        self.smile_seq = set()\n",
    "\n",
    "        # Hidden variables (for internal use only)\n",
    "        self._uniaccs = set()\n",
    "        self._unientries = set()\n",
    "        self._sequences = set()\n",
    "\n",
    "    @property\n",
    "    def invivo(self):\n",
    "        \"\"\"\n",
    "        Returns 1 if the TF has been profiled by in vivo methods,\n",
    "        or 0 otherwise.\n",
    "        @rtype = {int}\n",
    "        \"\"\"\n",
    "\n",
    "        # For simplicity, ChIP-seq ~ ChIP-exo ~ DAP-seq\n",
    "        if self.chip_atlas or self.cistromedb or self.gtrd or self.dap_seq or self.remap:\n",
    "            return(1)\n",
    "\n",
    "        return(0)\n",
    "\n",
    "    @property\n",
    "    def invitro(self):\n",
    "        \"\"\"\n",
    "        Returns the number of different experimental methods by\n",
    "        which a TF has been profiled in vitro.\n",
    "        @rtype = {int}\n",
    "        \"\"\"\n",
    "\n",
    "        n = 0\n",
    "\n",
    "        if self.ht_selex:\n",
    "            n += 1\n",
    "        # i.e. PBM\n",
    "        if self.cisbp or self.uniprobe:\n",
    "            n += 1\n",
    "        if self.smile_seq:\n",
    "            n += 1\n",
    "\n",
    "        return(n)\n",
    "\n",
    "    @property\n",
    "    def evidence(self):\n",
    "        \"\"\"\n",
    "        Returns the amount of evidence associated with a TF.\n",
    "        @rtype = {int}\n",
    "        \"\"\"\n",
    "        return(self.invivo+self.invitro)\n",
    "\n",
    "    def __str__(self):\n",
    "\n",
    "        string = \"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(\n",
    "            self.gene_name,\n",
    "            self.species,\n",
    "            self.uniacc,\n",
    "            # \";\".join(sorted([i for i in self._uniaccs if i != self.uniacc])),\n",
    "            self.unientry,\n",
    "            # \";\".join(sorted([i for i in self._unientries if i != self.unientry])),\n",
    "            self.status,\n",
    "            self.sequence,\n",
    "            # \";\".join(sorted([i for i in self._sequences if i != self.sequence])),\n",
    "            self.family,\n",
    "            self.cluster_num,\n",
    "            self.evidence,\n",
    "            # \";\".join(sorted([i for i in self._pfam_ids if i != self.pfam_id])),\n",
    "            # \";\".join(sorted(self.orthodb)),\n",
    "            self.jaspar_id,\n",
    "            \";\".join(sorted(self.hocomoco_id)),\n",
    "            \";\".join(sorted(self.chip_atlas)),\n",
    "            \";\".join(sorted(self.cistromedb)),\n",
    "            \";\".join(sorted(self.gtrd)),\n",
    "            \";\".join(sorted(self.remap)),\n",
    "            \";\".join(sorted(self.dap_seq)),\n",
    "            \";\".join(sorted(self.ht_selex)),\n",
    "            \";\".join(sorted(self.cisbp)),\n",
    "            \";\".join(sorted(self.uniprobe)),\n",
    "            \";\".join(sorted(self.smile_seq))\n",
    "        )\n",
    "\n",
    "        return(string)\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        string = \"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(\n",
    "            self.gene_name,\n",
    "            self.species,\n",
    "            self.uniacc,\n",
    "            # \";\".join(sorted([i for i in self._uniaccs if i != self.uniacc])),\n",
    "            self.unientry,\n",
    "            # \";\".join(sorted([i for i in self._unientries if i != self.unientry])),\n",
    "            self.status,\n",
    "            self.sequence,\n",
    "            # \";\".join(sorted([i for i in self._sequences if i != self.sequence])),\n",
    "            self.family,\n",
    "            self.cluster_num,\n",
    "            self.evidence,\n",
    "            # \";\".join(sorted([i for i in self._pfam_ids if i != self.pfam_id])),\n",
    "            # \";\".join(sorted(self.orthodb)),\n",
    "            self.jaspar_id,\n",
    "            \";\".join(sorted(self.hocomoco_id)),\n",
    "            \";\".join(sorted(self.chip_atlas)),\n",
    "            \";\".join(sorted(self.cistromedb)),\n",
    "            \";\".join(sorted(self.gtrd)),\n",
    "            \";\".join(sorted(self.remap)),\n",
    "            \";\".join(sorted(self.dap_seq)),\n",
    "            \";\".join(sorted(self.ht_selex)),\n",
    "            \";\".join(sorted(self.cisbp)),\n",
    "            \";\".join(sorted(self.uniprobe)),\n",
    "            \";\".join(sorted(self.smile_seq))\n",
    "        )\n",
    "\n",
    "        return(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------#\n",
    "# Paths       #\n",
    "#-------------#\n",
    "\n",
    "hocomoco_file = \"./Data/Databases/HOCOMOCO/HOCOMOCOv11_full_jaspar_format.txt\"\n",
    "chip_atlas_file = \"./Data/Experiments/ChIP-seq.ChIP-Atlas.tsv\"\n",
    "cistromedb_file = \"./Data/Experiments/ChIP-seq.CistromeDB.tsv\"\n",
    "gtrd_file = \"./Data/Experiments/ChIP-seq.GTRD.tsv\"\n",
    "remap_file = \"./Data/Experiments/ChIP-seq.ReMap2020.tsv\"\n",
    "dap_seq_file = \"./Data/Experiments/DAP-seq.PMID:27203113.tsv\"\n",
    "ht_selex_files = [\"./Data/Experiments/HT-SELEX.PMID:23332764.tsv\",\n",
    "                  \"./Data/Experiments/HT-SELEX.PMID:28473536.tsv\"]\n",
    "cisbp_txt_file = \"./Data/Databases/CisBP-2.0/PWMs.txt\"\n",
    "cisbp_tsv_file = \"./Data/Experiments/PBM.CisBP-2.0.tsv\"\n",
    "uniprobe_file = \"./Data/Experiments/PBM.UniPROBE.tsv\"\n",
    "smile_seq = \"./Data/Experiments/SMiLE-seq.PMID:28092692.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At1g23810\tArabidopsis thaliana\tQ9ZUB5\tQ9ZUB5_ARATH\tUnreviewed\tMAGGSKSPASSLEDGKAYVNAVKVALEEAEPAKYQEFLRLFHEVIARRMGMATFSARMQDLLKDHPSLCLGLNVMLAPEYQRAIPPEASEEFHKVVGRSVPRPEPTIDDATSYLIAVKEAFHDEPAKYEEMLKLLNDFKARRVNAASVIARVEELMKDHSNLLFGFCVFLSATTSFTTKLKAKFQGDGSQVVDSVLQIMRMYGEGNKSKHDAYQEIVALVQGHDDLVMELSQIFTDPSTRV\tUnknown\tNone\t0\tnan\t\t\t\t\t\t\t\t\t\t\n"
     ]
    }
   ],
   "source": [
    "#-------------#\n",
    "# Parse Data  #\n",
    "#-------------#\n",
    "\n",
    "# Initialize\n",
    "tfs = set()\n",
    "\n",
    "# For each line...\n",
    "for line in Jglobals.parse_tsv_file(\"./Data/Parsed/TFs.tab.gz\"):\n",
    "\n",
    "    # Initialize\n",
    "    tf = TF(line[0], line[1])\n",
    "\n",
    "    # Get UniProt Accession\n",
    "    uniaccs = line[2].split(\";\")\n",
    "    tf.uniacc = uniaccs[0]\n",
    "    tf._uniaccs.update(set(uniaccs))\n",
    "\n",
    "    # Get UniProt Entry\n",
    "    unientries = line[3].split(\";\")\n",
    "    tf.unientry = unientries[0]\n",
    "    tf._unientries.update(set(unientries))\n",
    "\n",
    "    # Get sequence\n",
    "    sequences = line[4].split(\";\")\n",
    "    tf.sequence = sequences[0]\n",
    "    tf._sequences.update(set(sequences))\n",
    "\n",
    "    # Get status (i.e. reviewed or not)\n",
    "    tf.status = line[5]\n",
    "\n",
    "    # Get family\n",
    "    tf.family = line[6]\n",
    "\n",
    "    # Get JASPAR ids\n",
    "    tf.jaspar_id = line[7]\n",
    "\n",
    "    # # Get orthoDB cluster\n",
    "    # codec = coreapi.codecs.CoreJSONCodec()\n",
    "    # for uniacc in tf._uniaccs:\n",
    "    #     json_file = os.path.join(args.orthodb, \"%s.json\" % uniacc)\n",
    "    #     if not os.path.exists(json_file):\n",
    "    #         client = coreapi.Client()\n",
    "    #         response = client.get(\n",
    "    #             \"https://www.orthodb.org/search?query=%s&level=2759&species=2759\" % uniacc)\n",
    "    #         json_obj = json.loads(codec.encode(response))\n",
    "    #         with open(json_file, \"w\") as j:\n",
    "    #             j.write(json.dumps(json_obj, sort_keys=True, indent=4, separators=(\",\", \": \")))\n",
    "    #     with open(json_file, \"r\") as j:  \n",
    "    #         json_obj = json.load(j)\n",
    "    #         for orthodb in json_obj[\"data\"]:\n",
    "    #             tf.orthodb.add(orthodb)\n",
    "\n",
    "    # Add TF to TFs\n",
    "    tfs.add(tf)\n",
    "\n",
    "print(next(iter(tfs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------#\n",
    "# HOCOMOCO    #\n",
    "#-------------#\n",
    "\n",
    "#  678 Homo sapiens\n",
    "#  451 Mus musculus\n",
    "\n",
    "# SMCA5_MOUSE # Not a TF: SWI/SNF-related matrix-associated actin-dependent regulator of chromatin subfamily A member 5\n",
    "# FUBP1_MOUSE # Not a TF: Far upstream element-binding protein 1\n",
    "# BRCA1_MOUSE # Not a TF: Breast cancer type 1 susceptibility protein homolog\n",
    "# EVI1_MOUSE  # Not a TF: Histone-lysine N-methyltransferase MECOM\n",
    "# TAF1_MOUSE  # Not a TF: Transcription initiation factor TFIID subunit 1\n",
    "# BRAC_MOUSE  # Not a valid UniProt Entry\n",
    "# HLTF_MOUSE  # Not a TF: Helicase-like transcription factor\n",
    "# TAF1_HUMAN  # Not a TF: Transcription initiation factor TFIID subunit 1\n",
    "# HLTF_HUMAN  # Not a TF: Helicase-like transcription factor\n",
    "# BRCA1_HUMAN # Not a TF: Breast cancer type 1 susceptibility protein\n",
    "# SMCA5_HUMAN # Not a TF: SWI/SNF-related matrix-associated actin-dependent regulator of chromatin subfamily A member 5\n",
    "# ZF64A_HUMAN # Not a valid UniProt Entry\n",
    "# BRAC_HUMAN  # Not a valid UniProt Entry\n",
    "# EVI1_HUMAN  # Not a TF: Histone-lysine N-methyltransferase MECOM\n",
    "# FUBP1_HUMAN # Not a TF: Far upstream element-binding protein 1\n",
    "# BPTF_HUMAN  # Not a TF: Nucleosome-remodeling factor subunit BPTF\n",
    "# CENPB_HUMAN # Not a TF: Major centromere autoantigen B\n",
    "# ZBT48_HUMAN # Not a valid UniProt Entry; should be TZAP_HUMAN\n",
    "\n",
    "# For each line...\n",
    "for line in Jglobals.parse_file(hocomoco_file):\n",
    "\n",
    "    if line.startswith(\">\"):\n",
    "\n",
    "        # Get unientry\n",
    "        m = re.search(\"(\\w+_(HUMAN|MOUSE))\", line)\n",
    "        unientry = m.group(1)\n",
    "\n",
    "        # For each TF...\n",
    "        for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "            if unientry in tf._unientries:\n",
    "                tf.hocomoco_id.add(line[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHR {'AHR_HUMAN.H11MO.0.B'}\n",
      "AIRE {'AIRE_HUMAN.H11MO.0.C'}\n",
      "ALX1 {'ALX1_HUMAN.H11MO.0.B'}\n",
      "ALX3 {'ALX3_HUMAN.H11MO.0.D'}\n",
      "ALX4 {'ALX4_HUMAN.H11MO.0.D'}\n",
      "AR {'ANDR_HUMAN.H11MO.0.A', 'ANDR_HUMAN.H11MO.1.A', 'ANDR_HUMAN.H11MO.2.A'}\n",
      "ARID3A {'ARI3A_HUMAN.H11MO.0.D'}\n",
      "ARID5B {'ARI5B_HUMAN.H11MO.0.C'}\n",
      "ARNT {'ARNT_HUMAN.H11MO.0.B'}\n",
      "ARNT2 {'ARNT2_HUMAN.H11MO.0.D'}\n",
      "...\n",
      "//\n",
      "Total genes: 1057\n",
      "Total feats: 1214\n"
     ]
    }
   ],
   "source": [
    "genes = 0\n",
    "feats = 0\n",
    "count = 0\n",
    "for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "    subtotal_feats = len(tf.hocomoco_id)\n",
    "    if subtotal_feats > 0:\n",
    "        genes += 1\n",
    "        feats += subtotal_feats\n",
    "        if count < 10:\n",
    "            print(tf.gene_name, tf.hocomoco_id)\n",
    "        elif count == 10:\n",
    "            print(\"...\")\n",
    "        else:\n",
    "            pass\n",
    "        count += 1\n",
    "print(\"//\")\n",
    "print(\"Total genes: %s\" % genes)\n",
    "print(\"Total feats: %s\" % feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------#\n",
    "# GTRD        #\n",
    "#-------------#\n",
    "\n",
    "#   71 Arabidopsis thaliana\n",
    "#  213 Caenorhabditis elegans\n",
    "#   11 Danio rerio\n",
    "#  249 Drosophila melanogaster\n",
    "# 1236 Homo sapiens\n",
    "#  513 Mus musculus\n",
    "#   12 Rattus norvegicus\n",
    "#  137 Saccharomyces cerevisiae\n",
    "#   32 Schizosaccharomyces pombe\n",
    "\n",
    "# For each line...\n",
    "for line in Jglobals.parse_tsv_file(gtrd_file):\n",
    "\n",
    "    # Inialize\n",
    "    experiment_id = line[0]\n",
    "    uniacc = line[1]\n",
    "\n",
    "    # For each TF...\n",
    "    for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "        if uniacc in tf._uniaccs:\n",
    "            tf.gtrd.add(experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11723 {'EXP043557'}\n",
      "ABF4 {'EXP041418'}\n",
      "ADD1 {'EXP044073'}\n",
      "ADNP {'EXP039553'}\n",
      "AEBP2 {'EXP040109'}\n",
      "AGL8 {'EXP040838'}\n",
      "AP1 {'EXP041282'}\n",
      "AR {'EXP038636', 'EXP038189', 'EXP040661', 'EXP038106', 'EXP040665', 'EXP038192', 'EXP038188', 'EXP038193', 'EXP037263', 'EXP040660', 'EXP038191', 'EXP036887', 'EXP037067'}\n",
      "ARF6 {'EXP041307'}\n",
      "ARID2 {'EXP040161'}\n",
      "...\n",
      "//\n",
      "Total genes: 814\n",
      "Total feats: 1958\n"
     ]
    }
   ],
   "source": [
    "genes = 0\n",
    "feats = 0\n",
    "count = 0\n",
    "for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "    subtotal_feats = len(tf.gtrd)\n",
    "    if subtotal_feats > 0:\n",
    "        genes += 1\n",
    "        feats += subtotal_feats\n",
    "        if count < 10:\n",
    "            print(tf.gene_name, tf.gtrd)\n",
    "        elif count == 10:\n",
    "            print(\"...\")\n",
    "        else:\n",
    "            pass\n",
    "        count += 1\n",
    "print(\"//\")\n",
    "print(\"Total genes: %s\" % genes)\n",
    "print(\"Total feats: %s\" % feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------#\n",
    "# DAP-seq     #\n",
    "#-------------#\n",
    "\n",
    "# For each line...\n",
    "for line in Jglobals.parse_tsv_file(dap_seq_file):\n",
    "\n",
    "    if line[0] == \"AvgSpotLen\":\n",
    "        continue\n",
    "\n",
    "    species = line[8]\n",
    "    sra_run = line[9]\n",
    "    gene = line[15]\n",
    "\n",
    "    if line[8] not in species:\n",
    "        continue\n",
    "\n",
    "    # For each TF...\n",
    "    for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "        if gene.upper() in tf.gene_name.upper():\n",
    "            tf.dap_seq.add(sra_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABF2 {'SRR2926839'}\n",
      "ABF2 {'SRR2926839'}\n",
      "ABI5 {'SRR2926840', 'SRR2926841'}\n",
      "ABR1 {'SRR2926082', 'SRR2926081'}\n",
      "AGL13 {'SRR2926463'}\n",
      "AGL15 {'SRR2926465', 'SRR2926464'}\n",
      "AGL16 {'SRR2926466', 'SRR2926467'}\n",
      "AGL42 {'SRR2926470'}\n",
      "AGL6 {'SRR2926472'}\n",
      "AGL61 {'SRR2926472'}\n",
      "...\n",
      "//\n",
      "Total genes: 474\n",
      "Total feats: 911\n"
     ]
    }
   ],
   "source": [
    "genes = 0\n",
    "feats = 0\n",
    "count = 0\n",
    "for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "    subtotal_feats = len(tf.dap_seq)\n",
    "    if subtotal_feats > 0:\n",
    "        genes += 1\n",
    "        feats += subtotal_feats\n",
    "        if count < 10:\n",
    "            print(tf.gene_name, tf.dap_seq)\n",
    "        elif count == 10:\n",
    "            print(\"...\")\n",
    "        else:\n",
    "            pass\n",
    "        count += 1\n",
    "print(\"//\")\n",
    "print(\"Total genes: %s\" % genes)\n",
    "print(\"Total feats: %s\" % feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------#\n",
    "# HT-SELEX    #\n",
    "#-------------#\n",
    "\n",
    "# For each file...\n",
    "for file_name in ht_selex_files:\n",
    "\n",
    "    m = re.search(\"HT-SELEX.PMID:(\\d+).tsv\", file_name)\n",
    "    pmid = int(m.group(1))\n",
    "\n",
    "    # For each line...\n",
    "    for line in Jglobals.parse_tsv_file(file_name):\n",
    "\n",
    "        if line[0] == \"Alias\":\n",
    "            continue\n",
    "\n",
    "        m = re.search(\"^([A-Za-z\\d]+)_\", line[0])\n",
    "        if m:\n",
    "\n",
    "            gene_name = m.group(1)\n",
    "\n",
    "            if pmid == 23332764:\n",
    "                sra_run = line[14]\n",
    "            else:\n",
    "                sra_run = line[19]\n",
    "\n",
    "            # For each TF...\n",
    "            for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "                # HT-SELEX data only available for human and mouse\n",
    "                if tf.species not in [\"Homo sapiens\", \"Mus musculus\"]:\n",
    "                    continue\n",
    "                if tf.gene_name == gene_name:\n",
    "                    tf.ht_selex.add(sra_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALX1 {'ERR1002054', 'ERR1002050', 'ERR1002056', 'ERR1002053', 'ERR1002051', 'ERR1002055', 'ERR1002052', 'ERR1002057'}\n",
      "ALX3 {'ERR1002071', 'ERR1002061', 'ERR1002064', 'ERR195525', 'ERR1002073', 'ERR1002066', 'ERR193771', 'ERR1002058', 'ERR1002067', 'ERR193772', 'ERR195526', 'ERR1010299', 'ERR1002059', 'ERR1002072', 'ERR1010665', 'ERR193773', 'ERR1002060', 'ERR1002069', 'ERR1002070', 'ERR1011404', 'ERR1002062', 'ERR195527', 'ERR1011038', 'ERR1002065', 'ERR195528', 'ERR1002063', 'ERR193770', 'ERR1002068'}\n",
      "ALX4 {'ERR1002075', 'ERR1002077', 'ERR194917', 'ERR1002080', 'ERR1002081', 'ERR1002076', 'ERR1002078', 'ERR1002079', 'ERR194920', 'ERR1002074', 'ERR194919', 'ERR194918'}\n",
      "AR {'ERR193972', 'ERR193678', 'ERR193679', 'ERR193970', 'ERR193680', 'ERR193971', 'ERR193973', 'ERR193681'}\n",
      "ARGFX {'ERR1002086', 'ERR1002082', 'ERR1002085', 'ERR1002087', 'ERR1002088', 'ERR1002084', 'ERR1002083', 'ERR1002089'}\n",
      "ARNT2 {'ERR1002094', 'ERR1010666', 'ERR1002093', 'ERR1010300', 'ERR1002092', 'ERR1002090', 'ERR1002096', 'ERR1011405', 'ERR1011039', 'ERR1002095', 'ERR1002091', 'ERR1002097'}\n",
      "ARNTL {'ERR1002100', 'ERR194922', 'ERR194923', 'ERR194924', 'ERR1002104', 'ERR1011406', 'ERR1002103', 'ERR194921', 'ERR1002099', 'ERR1002102', 'ERR1010667', 'ERR1002105', 'ERR1010301', 'ERR1011040', 'ERR1002101', 'ERR1002098'}\n",
      "ARX {'ERR1002112', 'ERR1002106', 'ERR195530', 'ERR1002110', 'ERR1002109', 'ERR1002107', 'ERR195531', 'ERR195529', 'ERR1002108', 'ERR1002111', 'ERR195532', 'ERR1002113'}\n",
      "ASCL1 {'ERR1002117', 'ERR1002116', 'ERR1002114', 'ERR1894980', 'ERR1002120', 'ERR1002121', 'ERR1002119', 'ERR1894976', 'ERR1002118', 'ERR1894979', 'ERR1894978', 'ERR1894982', 'ERR1002115', 'ERR1894977', 'ERR1894981', 'ERR1894983'}\n",
      "ASCL2 {'ERR1002136', 'ERR1002129', 'ERR1002131', 'ERR1002133', 'ERR1002134', 'ERR1002128', 'ERR1002122', 'ERR1002126', 'ERR1002125', 'ERR1002123', 'ERR1002127', 'ERR1002130', 'ERR1002124', 'ERR1002137', 'ERR1002132', 'ERR1002135'}\n",
      "...\n",
      "//\n",
      "Total genes: 673\n",
      "Total feats: 9462\n"
     ]
    }
   ],
   "source": [
    "genes = 0\n",
    "feats = 0\n",
    "count = 0\n",
    "for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "    subtotal_feats = len(tf.ht_selex)\n",
    "    if subtotal_feats > 0:\n",
    "        genes += 1\n",
    "        feats += subtotal_feats\n",
    "        if count < 10:\n",
    "            print(tf.gene_name, tf.ht_selex)\n",
    "        elif count == 10:\n",
    "            print(\"...\")\n",
    "        else:\n",
    "            pass\n",
    "        count += 1\n",
    "print(\"//\")\n",
    "print(\"Total genes: %s\" % genes)\n",
    "print(\"Total feats: %s\" % feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------#\n",
    "# CIS-BP      #\n",
    "#-------------#\n",
    "\n",
    "valid_matrix_ids = set()\n",
    "\n",
    "# Get valid PWMs\n",
    "for matrix_id in Jglobals.parse_file(cisbp_txt_file):\n",
    "    valid_matrix_ids.add(matrix_id)\n",
    "\n",
    "# For each line...\n",
    "for line in Jglobals.parse_file(cisbp_tsv_file):\n",
    "\n",
    "    matrix_ids = re.findall(\"(M\\d{5}_2.00)\", line)\n",
    "\n",
    "    if matrix_ids:\n",
    "\n",
    "        line = line.strip(\"\\n\").split(\"\\t\")\n",
    "        gene_name = line[1]\n",
    "        species = line[2].replace(\"_\", \" \")\n",
    "\n",
    "        # Skip inferred TFs\n",
    "        if line[3] != \"D\":\n",
    "            continue\n",
    "\n",
    "        # For each TF...\n",
    "        for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "            if tf.gene_name.upper() == gene_name.upper() and species == tf.species:\n",
    "                tf.cisbp.update(valid_matrix_ids.intersection(set(matrix_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABF1 {'M01772_2.00', 'M01773_2.00'}\n",
      "ABF1 {'M00001_2.00', 'M00902_2.00'}\n",
      "ABF2 {'M01771_2.00'}\n",
      "ABF2 {'M00069_2.00'}\n",
      "ABF3 {'M01782_2.00'}\n",
      "ABF4 {'M01057_2.00', 'M01056_2.00'}\n",
      "ABI5 {'M01776_2.00'}\n",
      "ABR1 {'M01051_2.00'}\n",
      "ACE2 {'M00033_2.00'}\n",
      "ADR1 {'M00021_2.00'}\n",
      "...\n",
      "//\n",
      "Total genes: 1199\n",
      "Total feats: 1541\n"
     ]
    }
   ],
   "source": [
    "genes = 0\n",
    "feats = 0\n",
    "count = 0\n",
    "for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "    subtotal_feats = len(tf.cisbp)\n",
    "    if subtotal_feats > 0:\n",
    "        genes += 1\n",
    "        feats += subtotal_feats\n",
    "        if count < 10:\n",
    "            print(tf.gene_name, tf.cisbp)\n",
    "        elif count == 10:\n",
    "            print(\"...\")\n",
    "        else:\n",
    "            pass\n",
    "        count += 1\n",
    "print(\"//\")\n",
    "print(\"Total genes: %s\" % genes)\n",
    "print(\"Total feats: %s\" % feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------#\n",
    "# UniPROBE    #\n",
    "#-------------#\n",
    "\n",
    "# For each line...\n",
    "for line in Jglobals.parse_tsv_file(uniprobe_file):\n",
    "   \n",
    "    if line[0] == \"Protein\":\n",
    "        continue\n",
    "\n",
    "    gene_name = line[0]\n",
    "    uniprobe_id = line[1]\n",
    "\n",
    "    # For each TF...\n",
    "    for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "        if tf.gene_name.upper() == gene_name.upper() and line[2] == tf.species:\n",
    "            tf.uniprobe.add(uniprobe_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABF1 {'UP00452'}\n",
      "AFT1 {'UP00344'}\n",
      "ARO80 {'UP00329'}\n",
      "ARX {'UP00584'}\n",
      "ASG1 {'UP00350'}\n",
      "Abd-B {'UP00503'}\n",
      "Ahctf1 {'UP01352'}\n",
      "Alx3 {'UP00108'}\n",
      "Alx4 {'UP00187'}\n",
      "Ar {'UP01353'}\n",
      "...\n",
      "//\n",
      "Total genes: 540\n",
      "Total feats: 558\n"
     ]
    }
   ],
   "source": [
    "genes = 0\n",
    "feats = 0\n",
    "count = 0\n",
    "for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "    subtotal_feats = len(tf.uniprobe)\n",
    "    if subtotal_feats > 0:\n",
    "        genes += 1\n",
    "        feats += subtotal_feats\n",
    "        if count < 10:\n",
    "            print(tf.gene_name, tf.uniprobe)\n",
    "        elif count == 10:\n",
    "            print(\"...\")\n",
    "        else:\n",
    "            pass\n",
    "        count += 1\n",
    "print(\"//\")\n",
    "print(\"Total genes: %s\" % genes)\n",
    "print(\"Total feats: %s\" % feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------#\n",
    "# SMiLE-seq   #\n",
    "#-------------#\n",
    "\n",
    "synonyms = {\n",
    "    \"CEBPb\": \"CEBPB\",\n",
    "    \"cFOS\": \"FOS\",\n",
    "    \"cFOSL2\": \"FOSL2\",\n",
    "    \"cJUN\": \"JUN\",\n",
    "    \"PPARa\": \"PPARA\",\n",
    "    \"PPARg\": \"PPARG\",\n",
    "    \"RXRa\": \"RXRA\",\n",
    "    \"RXRg\": \"RXRG\"            \n",
    "}\n",
    "\n",
    "# For each line...\n",
    "for line in Jglobals.parse_tsv_file(smile_seq):\n",
    "\n",
    "    if line[0] == \"Assay_Type\":\n",
    "        continue\n",
    "\n",
    "    sra_run = line[13]\n",
    "\n",
    "    for gene_name in line[16].split(\"_\")[0].split(\"-\"):\n",
    "        if gene_name in synonyms:\n",
    "            gene_name = synonyms[gene_name]\n",
    "\n",
    "        # For each TF...\n",
    "        for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "            if tf.gene_name.upper() == gene_name.upper() and line[12] == tf.species:\n",
    "                tf.smile_seq.add(sra_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARNTL {'SRR3405117', 'SRR3405116'}\n",
      "CEBPB {'SRR3405054'}\n",
      "CLOCK {'SRR3405117', 'SRR3405116'}\n",
      "CTCF {'SRR3405055', 'SRR3405078', 'SRR3405066'}\n",
      "EN1 {'SRR3405089'}\n",
      "Egr1 {'SRR3402436'}\n",
      "FLI1 {'SRR3405101'}\n",
      "FOS {'SRR3405133', 'SRR3405142', 'SRR3405143', 'SRR3405150', 'SRR3405132'}\n",
      "FOSB {'SRR3405144', 'SRR3405145', 'SRR3405151', 'SRR3405134'}\n",
      "FOSL1 {'SRR3405136', 'SRR3405152', 'SRR3405146'}\n",
      "...\n",
      "//\n",
      "Total genes: 70\n",
      "Total feats: 151\n"
     ]
    }
   ],
   "source": [
    "genes = 0\n",
    "feats = 0\n",
    "count = 0\n",
    "for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "    subtotal_feats = len(tf.smile_seq)\n",
    "    if subtotal_feats > 0:\n",
    "        genes += 1\n",
    "        feats += subtotal_feats\n",
    "        if count < 10:\n",
    "            print(tf.gene_name, tf.smile_seq)\n",
    "        elif count == 10:\n",
    "            print(\"...\")\n",
    "        else:\n",
    "            pass\n",
    "        count += 1\n",
    "print(\"//\")\n",
    "print(\"Total genes: %s\" % genes)\n",
    "print(\"Total feats: %s\" % feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JUN Homo sapiens 4 1 3\n",
      "PAX7 Homo sapiens 4 1 3\n",
      "MAX Homo sapiens 4 1 3\n",
      "Rxra Mus musculus 4 1 3\n",
      "FOS Homo sapiens 4 1 3\n",
      "BCL11B Homo sapiens 3 1 2\n",
      "BCL6 Homo sapiens 3 1 2\n",
      "CEBPB Homo sapiens 3 1 2\n",
      "CLOCK Homo sapiens 3 1 2\n",
      "CTCF Homo sapiens 3 1 2\n",
      "...\n",
      "//\n",
      "Homo sapiens 34\n",
      "Mus musculus 9\n"
     ]
    }
   ],
   "source": [
    "#-------------#\n",
    "# Triples     #\n",
    "#-------------#\n",
    "count = 0\n",
    "evidence = {}\n",
    "for tf in sorted(tfs, key=lambda x: x.evidence, reverse=True):\n",
    "    if tf.invivo > 0 and tf.invitro > 1:\n",
    "        evidence.setdefault(tf.species, 0)\n",
    "        evidence[tf.species] += 1\n",
    "        if count < 10:\n",
    "            print(tf.gene_name, tf.species, tf.evidence, tf.invivo, tf.invitro)\n",
    "        elif count == 10:\n",
    "            print(\"...\")\n",
    "        else:\n",
    "            pass\n",
    "        count += 1\n",
    "print(\"//\")\n",
    "for species in sorted(evidence):\n",
    "    print(species, evidence[species])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JUN Homo sapiens 4 1 3\n",
      "PAX7 Homo sapiens 4 1 3\n",
      "MAX Homo sapiens 4 1 3\n",
      "Rxra Mus musculus 4 1 3\n",
      "FOS Homo sapiens 4 1 3\n",
      "BCL11B Homo sapiens 3 1 2\n",
      "BCL6 Homo sapiens 3 1 2\n",
      "CEBPB Homo sapiens 3 1 2\n",
      "CLOCK Homo sapiens 3 1 2\n",
      "CTCF Homo sapiens 3 1 2\n",
      "...\n",
      "//\n",
      "Arabidopsis thaliana 114\n",
      "Caenorhabditis elegans 26\n",
      "Drosophila melanogaster 28\n",
      "Homo sapiens 214\n",
      "Mus musculus 58\n",
      "Saccharomyces cerevisiae 15\n"
     ]
    }
   ],
   "source": [
    "#-------------#\n",
    "# Duos        #\n",
    "#-------------#\n",
    "count = 0\n",
    "evidence = {}\n",
    "for tf in sorted(tfs, key=lambda x: x.evidence, reverse=True):\n",
    "    if tf.invivo > 0 and tf.invitro > 0:\n",
    "        evidence.setdefault(tf.species, 0)\n",
    "        evidence[tf.species] += 1\n",
    "        if count < 10:\n",
    "            print(tf.gene_name, tf.species, tf.evidence, tf.invivo, tf.invitro)\n",
    "        elif count == 10:\n",
    "            print(\"...\")\n",
    "        else:\n",
    "            pass\n",
    "        count += 1\n",
    "print(\"//\")\n",
    "for species in sorted(evidence):\n",
    "    print(species, evidence[species])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------#\n",
    "# Pfam        #\n",
    "#-------------#\n",
    "\n",
    "# The following code is adapted from:\n",
    "# https://github.com/wassermanlab/JASPAR-profile-inference/blob/master/files/get_files.py\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Alphabet import IUPAC\n",
    "import json\n",
    "from infer_profile import hmmAlign, hmmScan, _makeSeqFile\n",
    "\n",
    "# Initialize\n",
    "seq_file = \".seq.fasta\"\n",
    "hmm_database = os.path.join(jaspar_dir, \"files\", \"pfam-DBDs\", \"all_DBDs.hmm\")\n",
    "\n",
    "# Change dir\n",
    "os.chdir(sys.path[1])\n",
    "os.chdir(os.path.abspath(\"./Data/Pfam/\"))\n",
    "\n",
    "# Skip if JSON file already exists\n",
    "json_file = \"TFs.json\"\n",
    "if not os.path.exists(json_file):\n",
    "\n",
    "    # Initialize\n",
    "    pfams = {}\n",
    "\n",
    "    # For each TF...\n",
    "    for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "\n",
    "        # Initialize\n",
    "        uniacc = tf.uniacc\n",
    "        pfams.setdefault(uniacc, [])\n",
    "\n",
    "        # Make seq file\n",
    "        seq = Seq(tf.sequence, IUPAC.protein)\n",
    "        seq_record = SeqRecord(seq, id=uniacc, name=uniacc, description=uniacc)\n",
    "        _makeSeqFile(seq_record, seq_file)\n",
    "\n",
    "        # For each DBD...\n",
    "        for pfam_ac, start, end, evalue in hmmScan(seq_file, hmm_database, non_overlapping_domains=True):\n",
    "\n",
    "            # Initialize\n",
    "            hmm_file = os.path.join(jaspar_dir, \"files\", \"pfam-DBDs\", \"%s.hmm\" % pfam_ac)\n",
    "\n",
    "            # Make seq file\n",
    "            sub_seq = seq[start:end]\n",
    "            seq_record = SeqRecord(sub_seq, id=uniacc, name=uniacc, description=uniacc)\n",
    "            _makeSeqFile(seq_record, seq_file)\n",
    "\n",
    "            # Add DBDs\n",
    "            alignment = hmmAlign(seq_file, hmm_file)\n",
    "            pfams[uniacc].append((pfam_ac, alignment, start+1, end, evalue))\n",
    "\n",
    "    # Write\n",
    "    Jglobals.write(json_file, json.dumps(pfams, sort_keys=True, indent=4, separators=(\",\", \": \")))\n",
    "\n",
    "    # Remove seq file\n",
    "    if os.path.exists(seq_file):\n",
    "        os.remove(seq_file)\n",
    "\n",
    "# Change dir back\n",
    "os.chdir(sys.path[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------#\n",
    "# Cluster     #\n",
    "#-------------#\n",
    "\n",
    "# The following code is adapted from:\n",
    "# https://github.com/wassermanlab/JASPAR-profile-inference/blob/master/finfer_profile.py\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from infer_profile import _fetchXs, _filter_results_by_Rost, _removeLowercase\n",
    "\n",
    "def get_members(tf):\n",
    "\n",
    "    # Initialize\n",
    "    members = []\n",
    "    tf_DBDs = [dbd[0] for dbd in pfams[tf.uniacc]]\n",
    "    tf_alignments = [dbd[1] for dbd in pfams[tf.uniacc]]\n",
    "    seq = Seq(tf.sequence, IUPAC.protein)\n",
    "    seq_record = SeqRecord(seq, id=tf.uniacc, name=tf.uniacc, description=tf.uniacc)\n",
    "    fasta_file = os.path.join(clusters_dir, \"%s.fasta\" % species_dict[tf.species])\n",
    "\n",
    "    # Get cut-offs on the percentage of sequence identity\n",
    "    cutoffs = {}\n",
    "    for pfam_ac in pfam_cutoffs:\n",
    "        if pfam_cutoffs[pfam_ac][0] in tf_DBDs:\n",
    "            cutoffs.setdefault(pfam_cutoffs[pfam_ac][0], pfam_cutoffs[pfam_ac][1])\n",
    "\n",
    "    # BLAST+ search\n",
    "    blast_results = BLAST(seq_record, fasta_file)\n",
    "\n",
    "    # Filter results\n",
    "    for filtered_result in sorted(_filter_results_by_Rost(blast_results), key=lambda x: x[5], reverse=True):\n",
    "\n",
    "        if filtered_result[1] in clusters:\n",
    "            continue\n",
    "\n",
    "        # Both TFs have same DBD composition\n",
    "        if [dbd[0] for dbd in pfams[filtered_result[1]]] == tf_DBDs:\n",
    "\n",
    "            # Inference: percentage of sequence identity\n",
    "            pids = []\n",
    "            pid_cutoffs = []\n",
    "            alignments = [dbd[1] for dbd in pfams[filtered_result[1]]]\n",
    "            for a in range(len(alignments)):\n",
    "                s1 = _removeLowercase(tf_alignments[a])\n",
    "                s2 = _removeLowercase(alignments[a])\n",
    "                pids.append(sum(_fetchXs(s1, s2))/float(len(s1)))\n",
    "                pid_cutoffs.append(pids[-1] >= cutoffs[tf_DBDs[a]])\n",
    "            if True in pid_cutoffs:\n",
    "                members.append(filtered_result[1])\n",
    "\n",
    "    return(members)\n",
    "\n",
    "def BLAST(seq_record, fasta_file):\n",
    "\n",
    "    # Initialize\n",
    "    blast_results = set()\n",
    "    outfmt = \"sseqid pident length qstart qend sstart send evalue bitscore ppos qlen slen\"\n",
    "\n",
    "    # Run BLAST+\n",
    "    cmd = \"blastp -db %s -outfmt \\\"6 %s\\\"\" % (fasta_file, outfmt)\n",
    "    process = subprocess.Popen([cmd], shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    fasta_sequence = \">%s\\n%s\" % (seq_record.id, seq_record.seq)\n",
    "    process.stdin.write(fasta_sequence.encode())\n",
    "    (blast_records, blast_errors) = process.communicate()\n",
    "\n",
    "    # For each BLAST+ record...\n",
    "    for blast_record in blast_records.decode(\"utf-8\").split(\"\\n\"):\n",
    "\n",
    "        # Custom BLAST+ record:\n",
    "        # (1) identifier of target sequence;\n",
    "        # (2) percentage of identical matches;\n",
    "        # (3) alignment length;\n",
    "        # (4-5, 6-7) start and end-position in query and in target;\n",
    "        # (8) E-value;\n",
    "        # (9) bit score;\n",
    "        # (10) percentage of positive-scoring matches; and\n",
    "        # (4-7, 11, 12) joint coverage (i.e. square root of the coverage\n",
    "        # on the query and the target).\n",
    "        blast_record = blast_record.split(\"\\t\")\n",
    "\n",
    "        # Skip if not a BLAST+ record\n",
    "        if len(blast_record) != 12: continue\n",
    "\n",
    "        # Get BLAST+ record\n",
    "        target_id = blast_record[0]\n",
    "        percent_identities = float(blast_record[1])\n",
    "        alignment_length = int(blast_record[2])\n",
    "        query_start_end = \"%s-%s\" % (blast_record[3], blast_record[4])\n",
    "        target_start_end = \"%s-%s\" % (blast_record[5], blast_record[6])\n",
    "        e_value = float(blast_record[7])\n",
    "        score = float(blast_record[8])\n",
    "        percent_similarity = float(blast_record[9])\n",
    "        query_aligned_residues = int(blast_record[4]) - int(blast_record[3]) + 1\n",
    "        query_length = float(blast_record[10])\n",
    "        target_aligned_residues = int(blast_record[6]) - int(blast_record[5]) + 1\n",
    "        target_length = float(blast_record[11])\n",
    "        query_coverage = query_aligned_residues * 100 / query_length\n",
    "        target_coverage = target_aligned_residues * 100 / target_length\n",
    "        joint_coverage = math.sqrt(query_coverage * target_coverage)\n",
    "\n",
    "        # Add BLAST+ record to search results\n",
    "        blast_results.add((seq_record.id, target_id, query_start_end, target_start_end, e_value, score, percent_identities, alignment_length, percent_similarity, joint_coverage))\n",
    "\n",
    "    # Return results sorted by score\n",
    "    return(list(sorted(blast_results, key=lambda x: x[-1], reverse=True)))\n",
    "\n",
    "# Initialize\n",
    "cluster = 0\n",
    "clusters = {}\n",
    "clusters_dir = \"./Data/Clusters/\"\n",
    "species_dict = {\n",
    "    \"Arabidopsis thaliana\": \"plants\",\n",
    "    \"Caenorhabditis elegans\": \"nematodes\",\n",
    "    \"Drosophila melanogaster\": \"insects\",\n",
    "    \"Homo sapiens\": \"vertebrates\",\n",
    "    \"Mus musculus\": \"vertebrates\",\n",
    "    \"Saccharomyces cerevisiae\": \"fungi\"\n",
    "}\n",
    "with open(\"./Data/Pfam/TFs.json\") as f:\n",
    "    pfams = json.load(f)\n",
    "with open(os.path.join(jaspar_dir, \"files\", \"pfam-DBDs.json\")) as f:\n",
    "    pfam_cutoffs = json.load(f)\n",
    "\n",
    "for species in species_dict:\n",
    "\n",
    "    # Initialize\n",
    "    fasta_file = os.path.join(clusters_dir, \"%s.fasta\" % species_dict[species])\n",
    "\n",
    "    if not os.path.exists(\"%s.psq\" % fasta_file):\n",
    "\n",
    "        # For each TF...\n",
    "        for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "\n",
    "            if tf.species != species:\n",
    "                continue\n",
    "\n",
    "            Jglobals.write(fasta_file, \">%s\\n%s\" % (tf.uniacc, tf.sequence))\n",
    "\n",
    "for species in species_dict:\n",
    "\n",
    "    # Initialize\n",
    "    fasta_file = os.path.join(clusters_dir, \"%s.fasta\" % species_dict[species])\n",
    "\n",
    "    if not os.path.exists(\"%s.psq\" % fasta_file):\n",
    "\n",
    "        # Make BLAST+ database\n",
    "        cmd = \"makeblastdb -in %s -dbtype prot\" % fasta_file\n",
    "        process = subprocess.run([cmd], shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "# Skip if JSON file already exists\n",
    "json_file = os.path.join(clusters_dir, \"clusters.json\")\n",
    "if not os.path.exists(json_file):\n",
    "\n",
    "    # Start with TFs supported by both in vivo and in vitro evidence\n",
    "    for tf in sorted(tfs, key=lambda x: x.evidence, reverse=True):\n",
    "\n",
    "        if tf.uniacc in clusters:\n",
    "            continue\n",
    "\n",
    "        if tf.invivo > 0 and tf.invitro > 0:\n",
    "\n",
    "            # Get cluster members\n",
    "            members = get_members(tf)\n",
    "\n",
    "            if members:\n",
    "\n",
    "                cluster += 1\n",
    "\n",
    "                for member in members:\n",
    "                    clusters.setdefault(member, cluster)\n",
    "\n",
    "    # Then focus on TFs supported by one kind of evidence\n",
    "    for tf in sorted(tfs, key=lambda x: x.evidence, reverse=True):\n",
    "\n",
    "        if tf.uniacc in clusters:\n",
    "            continue\n",
    "\n",
    "        if tf.evidence > 0:\n",
    "\n",
    "            # Get cluster members\n",
    "            members = get_members(tf)\n",
    "\n",
    "            if members:\n",
    "\n",
    "                cluster += 1\n",
    "\n",
    "                for member in members:\n",
    "                    clusters.setdefault(member, cluster)\n",
    "\n",
    "    # Write\n",
    "    Jglobals.write(json_file, json.dumps(clusters, sort_keys=True, indent=4, separators=(\",\", \": \")))\n",
    "\n",
    "with open(json_file) as f:\n",
    "    clusters = json.load(f)\n",
    "\n",
    "for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "    if tf.uniacc in clusters:\n",
    "        tf.cluster_num = clusters[tf.uniacc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JUN\tHomo sapiens\tP05412\tJUN_HUMAN\tReviewed\tMTAKMETTFYDDALNASFLPSESGPYGYSNPKILKQSMTLNLADPVGSLKPHLRAKNSDLLTSPDVGLLKLASPELERLIIQSSNGHITTTPTPTQFLCPKNVTDEQEGFAEGFVRALAELHSQNTLPSVTSAAQPVNGAGMVAPAVASVAGGSGSGGFSASLHSEPPVYANLSNFNPGALSSGGGAPSYGAAGLAFPAQPQQQQQPPHHLPQQMPVQHPRLQALKEEPQTVPEMPGETPPLSPIDMESQERIKAERKRMRNRIAASKCRKRKLERIARLEEKVKTLKAQNSELASTANMLREQVAQLKQKVMNHVNSGCQLMLTQQLQTF\tbZIP\t1\t4\tMA0099.2;MA0099.3;MA0462.1;MA0462.2;MA0488.1;MA0489.1;MA1126.1;MA1127.1;MA1128.1;MA1129.1;MA1130.1;MA1131.1;MA1132.1;MA1133.1\tJUN_HUMAN.H11MO.0.A\t\t\tEXP037742;EXP038429;EXP039416;EXP039444;EXP039488;EXP039496;EXP039507;EXP039519;EXP039545;EXP039637;EXP039869;EXP039941;EXP039950;EXP039981;EXP040017;EXP040243;EXP040248;EXP040263;EXP040265;EXP040320;EXP040322\t\t\tERR1004354;ERR1004355;ERR1004356;ERR1004357;ERR1004358;ERR1004359;ERR1004360;ERR1004361\t\tUP00426\tSRR3405057;SRR3405058;SRR3405059;SRR3405132;SRR3405133;SRR3405134;SRR3405136;SRR3405137;SRR3405138;SRR3405140\n",
      "PAX7\tHomo sapiens\tP23759\tPAX7_HUMAN\tReviewed\tMAALPGTVPRMMRPAPGQNYPRTGFPLEVSTPLGQGRVNQLGGVFINGRPLPNHIRHKIVEMAHHGIRPCVISRQLRVSHGCVSKILCRYQETGSIRPGAIGGSKPRQVATPDVEKKIEEYKRENPGMFSWEIRDRLLKDGHCDRSTVPSGLVSSISRVLRIKFGKKEEEDEADKKEDDGEKKAKHSIDGILGDKGNRLDEGSDVESEPDLPLKRKQRRSRTTFTAEQLEELEKAFERTHYPDIYTREELAQRTKLTEARVQVWFSNRRARWRKQAGANQLAAFNHLLPGGFPPTGMPTLPPYQLPDSTYPTTTISQDGGSTVHRPQPLPPSTMHQGGLAAAAAAADTSSAYGARHSFSSYSDSFMNPAAPSNHMNPVSNGLSPQVMSILGNPSAVPPQPQADFSISPLHGGLDSATSISASCSQRADSIKPGDSLPTSQAYCPPTYSTTGYSVDPVAGYQYGQYGQTAVDYLAKNVSLSTQRRMKLGEHSAVLGLLPVETGQAY\tHomeodomain,Paired box\t2\t4\tMA0680.1\tPAX7_HUMAN.H11MO.0.D\t\t\tEXP038641\t\t\tERR1005486;ERR1005487;ERR1005488;ERR1005489;ERR1005490;ERR1005491;ERR1005492;ERR1005493;ERR1005494;ERR1005495;ERR1005496;ERR1005497;ERR1005498;ERR1005499;ERR1005500;ERR1005501;ERR1010556;ERR1010922;ERR1011295;ERR1011661;ERR193886;ERR193887;ERR193888;ERR193889;ERR195393;ERR195394;ERR195395;ERR195396\tM00345_2.00;M00346_2.00\tUP00612\tSRR3405074\n",
      "MAX\tHomo sapiens\tP61244\tMAX_HUMAN\tReviewed\tMSDNDDIEVESDEEQPRFQSAADKRAHHNALERKRRDHIKDSFHSLRDSVPSLQGEKASRAQILDKATEYIQYMRRKNHTHQQDIDDLKRQNALLEQQVRALEKARSSAQLQTNYPSSDNSLYTNAKGSTISAFDGGSDSSSESEPEEPQSRKKLRMEAS\tbHLH\t3\t4\tMA0058.1;MA0058.2;MA0058.3;MA0059.1\tMAX_HUMAN.H11MO.0.A\t\t\tEXP039669;EXP039825\t\t\tERR1004670;ERR1004671;ERR1004672;ERR1004673;ERR1004674;ERR1004675;ERR1004676;ERR1004677;ERR1004678;ERR1004679;ERR1004680;ERR1004681;ERR1004682;ERR1004683;ERR1004684;ERR1004685;ERR1010504;ERR1010505;ERR1010870;ERR1010871;ERR1011243;ERR1011244;ERR1011609;ERR1011610;ERR1895240;ERR1895241;ERR1895242;ERR1895243;ERR1895244;ERR1895245;ERR1895246;ERR1895247;ERR1905265;ERR1905266;ERR195305;ERR195306;ERR195307;ERR195308\tM00938_2.00;M01497_2.00;M01498_2.00\t\tSRR3405065;SRR3405067\n",
      "Rxra\tMus musculus\tP28700\tRXRA_MOUSE\tReviewed\tMDTKHFLPLDFSTQVNSSSLNSPTGRGSMAVPSLHPSLGPGIGSPLGSPGQLHSPISTLSSPINGMGPPFSVISSPMGPHSMSVPTTPTLGFGTGSPQLNSPMNPVSSTEDIKPPLGLNGVLKVPAHPSGNMASFTKHICAICGDRSSGKHYGVYSCEGCKGFFKRTVRKDLTYTCRDNKDCLIDKRQRNRCQYCRYQKCLAMGMKREAVQEERQRGKDRNENEVESTSSANEDMPVEKILEAELAVEPKTETYVEANMGLNPSSPNDPVTNICQAADKQLFTLVEWAKRIPHFSELPLDDQVILLRAGWNELLIASFSHRSIAVKDGILLATGLHVHRNSAHSAGVGAIFDRVLTELVSKMRDMQMDKTELGCLRAIVLFNPDSKGLSNPAEVEALREKVYASLEAYCKHKYPEQPGRFAKLLLRLPALRSIGLKCLEHLFFFKLIGDTPIDTFLMEMLEAPHQAT\tNuclear receptor\t4\t4\tMA0065.2;MA0494.1;MA0512.1;MA0512.2\tRXRA_MOUSE.H11MO.0.A;RXRA_MOUSE.H11MO.1.A;RXRA_MOUSE.H11MO.2.A\t\t\tEXP036879;EXP036881;EXP036925\t\t\tERR194855;ERR194856;ERR194857\tM00181_2.00\tUP00053\tSRR3402439;SRR3402440;SRR3402448;SRR3402449\n",
      "FOS\tHomo sapiens\tP01100\tFOS_HUMAN\tReviewed\tMMFSGFNADYEASSSRCSSASPAGDSLSYYHSPADSFSSMGSPVNAQDFCTDLAVSSANFIPTVTAISTSPDLQWLVQPALVSSVAPSQTRAPHPFGVPAPSAGAYSRAGVVKTMTGGRAQSIGRRGKVEQLSPEEEEKRRIRRERNKMAAAKCRNRRRELTDTLQAETDQLEDEKSALQTEIANLLKEKEKLEFILAAHRPACKIPDDLGFPEEMSVASLDLTGGLPEVATPESEEAFTLPLLNDPEPKPSVEPVKSISSMELKTEPFDDFLFPASSRPSGSETARSVPDMDLSGSFYAADWEPLHSGSLGMGPMATELEPLCTPVVTCTPSCTAYTSSFVFTYPEADSFPSCAAAHRKGSSSNEPSSDSLSSPTLLAL\tbZIP\t5\t4\tMA0099.2;MA0099.3;MA0476.1;MA1126.1;MA1134.1;MA1141.1\tFOS_HUMAN.H11MO.0.A\t\t\tEXP039888;EXP040137\t\t\tERR1003202;ERR1003203;ERR1003204;ERR1003205;ERR1003206;ERR1003207;ERR1003208;ERR1003209;ERR1010380;ERR1010746;ERR1011119;ERR1011485\t\tUP00425\tSRR3405132;SRR3405133;SRR3405142;SRR3405143;SRR3405150\n",
      "BCL11B\tHomo sapiens\tQ9C0K0\tBC11B_HUMAN\tReviewed\tMSRRKQGNPQHLSQRELITPEADHVEAAILEEDEGLEIEEPSGLGLMVGGPDPDLLTCGQCQMNFPLGDILVFIEHKRKQCGGSLGACYDKALDKDSPPPSSRSELRKVSEPVEIGIQVTPDEDDHLLSPTKGICPKQENIAGPCRPAQLPAVAPIAASSHPHSSVITSPLRALGALPPCLPLPCCSARPVSGDGTQGEGQTEAPFGCQCQLSGKDEPSSYICTTCKQPFNSAWFLLQHAQNTHGFRIYLEPGPASSSLTPRLTIPPPLGPEAVAQSPLMNFLGDSNPFNLLRMTGPILRDHPGFGEGRLPGTPPLFSPPPRHHLDPHRLSAEEMGLVAQHPSAFDRVMRLNPMAIDSPAMDFSRRLRELAGNSSTPPPVSPGRGNPMHRLLNPFQPSPKSPFLSTPPLPPMPPGGTPPPQPPAKSKSCEFCGKTFKFQSNLIVHRRSHTGEKPYKCQLCDHACSQASKLKRHMKTHMHKAGSLAGRSDDGLSAASSPEPGTSELAGEGLKAADGDFRHHESDPSLGHEPEEEDEEEEEEEEELLLENESRPESSFSMDSELSRNRENGGGGVPGVPGAGGGAAKALADEKALVLGKVMENVGLGALPQYGELLADKQKRGAFLKRAAGGGDAGDDDDAGGCGDAGAGGAVNGRGGGFAPGTEPFPGLFPRKPAPLPSPGLNSAAKRIKVEKDLELPPAALIPSENVYSQWLVGYAASRHFMKDPFLGFTDARQSPFATSSEHSSENGSLRFSTPPGDLLDGGLSGRSGTASGGSTPHLGGPGPGRPSSKEGRRSDTCEYCGKVFKNCSNLTVHRRSHTGERPYKCELCNYACAQSSKLTRHMKTHGQIGKEVYRCDICQMPFSVYSTLEKHMKKWHGEHLLTNDVKIEQAERS\tC2H2 ZF\t29\t3\tnan\t\t\t\tEXP039533\t\t\tERR1002298;ERR1002299;ERR1002300;ERR1002301;ERR1002302;ERR1002303;ERR1002304;ERR1002305\tM00986_2.00;M00987_2.00;M00988_2.00\tUP01465\t\n",
      "BCL6\tHomo sapiens\tP41182\tBCL6_HUMAN\tReviewed\tMASPADSCIQFTRHASDVLLNLNRLRSRDILTDVVIVVSREQFRAHKTVLMACSGLFYSIFTDQLKCNLSVINLDPEINPEGFCILLDFMYTSRLNLREGNIMAVMATAMYLQMEHVVDTCRKFIKASEAEMVSAIKPPREEFLNSRMLMPQDIMAYRGREVVENNLPLRSAPGCESRAFAPSLYSGLSTPPASYSMYSHLPVSSLLFSDEEFRDVRMPVANPFPKERALPCDSARPVPGEYSRPTLEVSPNVCHSNIYSPKETIPEEARSDMHYSVAEGLKPAAPSARNAPYFPCDKASKEEERPSSEDEIALHFEPPNAPLNRKGLVSPQSPQKSDCQPNSPTESCSSKNACILQASGSPPAKSPTDPKACNWKKYKFIVLNSLNQNAKPEGPEQAELGRLSPRAYTAPPACQPPMEPENLDLQSPTKLSASGEDSTIPQASRLNNIVNRSMTGSPRSSSESHSPLYMHPPKCTSCGSQSPQHAEMCLHTAGPTFPEEMGETQSEYSDSSCENGAFFCNECDCRFSEEASLKRHTLQTHSDKPYKCDRCQASFRYKGNLASHKTVHTGEKPYRCNICGAQFNRPANLKTHTRIHSGEKPYKCETCGARFVQVAHLRAHVLIHTGEKPYPCEICGTRFRHLQTLKSHLRIHTGEKPYHCEKCNLHFRHKSQLRLHLRQKHGAITNTKVQYRVSATDLPPELPKAC\tC2H2 ZF\t30\t3\tMA0463.2\tBCL6_HUMAN.H11MO.0.A\t\t\tEXP039932\t\t\tERR1002306;ERR1002307;ERR1002308;ERR1002309;ERR1002310;ERR1002311;ERR1002312;ERR1002313;ERR1010314;ERR1010680;ERR1011053;ERR1011419\tM00227_2.00;M00228_2.00\tUP00585\t\n",
      "CEBPB\tHomo sapiens\tP17676\tCEBPB_HUMAN\tReviewed\tMQRLVAWDPACLPLPPPPPAFKSMEVANFYYEADCLAAAYGGKAAPAAPPAARPGPRPPAGELGSIGDHERAIDFSPYLEPLGAPQAPAPATATDTFEAAPPAPAPAPASSGQHHDFLSDLFSDDYGGKNCKKPAEYGYVSLGRLGAAKGALHPGCFAPLHPPPPPPPPPAELKAEPGFEPADCKRKEEAGAPGGGAGMAAGFPYALRAYLGYQAVPSGSSGSLSTSSSSSPPGTPSPADAKAPPTACYAGAAPAPSQVKSKAKKTVDKHSDEYKIRRERNNIAVRKSRDKAKMRNLETQHKVLELTAENERLQKKVEQLSRELSTLRNLFKQLPEPLLASSGHC\tbZIP\t24\t3\tMA0466.1;MA0466.2\tCEBPB_HUMAN.H11MO.0.A\t\t\tEXP037879;EXP037880;EXP037881;EXP037882;EXP037966;EXP039491;EXP039499;EXP039535;EXP039590;EXP039591;EXP039834;EXP039871;EXP039935;EXP039961;EXP040030;EXP040158;EXP040177;EXP040190;EXP040801\t\t\tERR1002402;ERR1002403;ERR1002404;ERR1002405;ERR1002406;ERR1002407;ERR1002408;ERR1002409;ERR1010321;ERR1010687;ERR1011060;ERR1011426;ERR1895008;ERR1895009;ERR1895010;ERR1895011;ERR1895012;ERR1895013;ERR1895014;ERR1895015;ERR1905264;ERR194929;ERR194930;ERR194931;ERR194932;ERR194933;ERR194934;ERR194935;ERR194936\t\t\tSRR3405054\n",
      "CLOCK\tHomo sapiens\tO15516\tCLOCK_HUMAN\tReviewed\tMLFTVSCSKMSSIVDRDDSSIFDGLVEEDDKDKAKRVSRNKSEKKRRDQFNVLIKELGSMLPGNARKMDKSTVLQKSIDFLRKHKEITAQSDASEIRQDWKPTFLSNEEFTQLMLEALDGFFLAIMTDGSIIYVSESVTSLLEHLPSDLVDQSIFNFIPEGEHSEVYKILSTHLLESDSLTPEYLKSKNQLEFCCHMLRGTIDPKEPSTYEYVKFIGNFKSLNSVSSSAHNGFEGTIQRTHRPSYEDRVCFVATVRLATPQFIKEMCTVEEPNEEFTSRHSLEWKFLFLDHRAPPIIGYLPFEVLGTSGYDYYHVDDLENLAKCHEHLMQYGKGKSCYYRFLTKGQQWIWLQTHYYITYHQWNSRPEFIVCTHTVVSYAEVRAERRRELGIEESLPETAADKSQDSGSDNRINTVSLKEALERFDHSPTPSASSRSSRKSSHTAVSDPSSTPTKIPTDTSTPPRQHLPAHEKMVQRRSSFSSQSINSQSVGSSLTQPVMSQATNLPIPQGMSQFQFSAQLGAMQHLKDQLEQRTRMIEANIHRQQEELRKIQEQLQMVHGQGLQMFLQQSNPGLNFGSVQLSSGNSSNIQQLAPINMQGQVVPTNQIQSGMNTGHIGTTQHMIQQQTLQSTSTQSQQNVLSGHSQQTSLPSQTQSTLTAPLYNTMVISQPAAGSMVQIPSSMPQNSTQSAAVTTFTQDRQIRFSQGQQLVTKLVTAPVACGAVMVPSTMLMGQVVTAYPTFATQQQQSQTLSVTQQQQQQSSQEQQLTSVQQPSQAQLTQPPQQFLQTSRLLHGNPSTQLILSAAFPLQQSTFPQSHHQQHQSQQQQQLSRHRTDSLPDPSKVQPQ\tbHLH\t31\t3\tMA0819.1\tCLOCK_HUMAN.H11MO.0.C\t\t\tEXP040233;EXP040305\t\t\tERR1002442;ERR1002443;ERR1002444;ERR1002445;ERR1002446;ERR1002447;ERR1002448;ERR1002449;ERR1010325;ERR1010691;ERR1011064;ERR1011430;ERR195557;ERR195558;ERR195559;ERR195560\t\t\tSRR3405116;SRR3405117\n",
      "CTCF\tHomo sapiens\tP49711\tCTCF_HUMAN\tReviewed\tMEGDAVEAIVEESETFIKGKERKTYQRRREGGQEEDACHLPQNQTDGGEVVQDVNSSVQMVMMEQLDPTLLQMKTEVMEGTVAPEAEAAVDDTQIITLQVVNMEEQPINIGELQLVQVPVPVTVPVATTSVEELQGAYENEVSKEGLAESEPMICHTLPLPEGFQVVKVGANGEVETLEQGELPPQEDPSWQKDPDYQPPAKKTKKTKKSKLRYTEEGKDVDVSVYDFEEEQQEGLLSEVNAEKVVGNMKPPKPTKIKKKGVKKTFQCELCSYTCPRRSNLDRHMKSHTDERPHKCHLCGRAFRTVTLLRNHLNTHTGTRPHKCPDCDMAFVTSGELVRHRRYKHTHEKPFKCSMCDYASVEVSKLKRHIRSHTGERPFQCSLCSYASRDTYKLKRHMRTHSGEKPYECYICHARFTQSGTMKMHILQKHTENVAKFHCPHCDTVIARKSDLGVHLRKQHSYIEQGKKCRYCDAVFHERYALIQHQKSHKNEKRFKCDQCDYACRQERHMIMHKRTHTGEKPYACSHCDKTFRQKQLLDMHFKRYHDPNFVPAAFVCSKCGKTFTRRNTMARHADNCAGPDGVEGENGGETKKSKRGRKRKMRSKKEDSSDSENAEPDLDDNEDEEEPAVEIEPEPEPQPVTPAPPPAKKRRGRPPGRTNQPKQNQPTAIIQVEDQNTGAIENIIVEVKKEPDAEPAEGEEEEAQPAATDAPNGDLTPEMILSMMDR\tC2H2 ZF\t32\t3\tMA0139.1\tCTCF_HUMAN.H11MO.0.A\t\t\tEXP036890;EXP037459;EXP037532;EXP037799;EXP037800;EXP037801;EXP037952;EXP038267;EXP038268;EXP038623;EXP038624;EXP039310;EXP039321;EXP039328;EXP039334;EXP039370;EXP039379;EXP039386;EXP039388;EXP039398;EXP039402;EXP039406;EXP039407;EXP039413;EXP039434;EXP039438;EXP039453;EXP039454;EXP039457;EXP039463;EXP039468;EXP039469;EXP039472;EXP039474;EXP039475;EXP039487;EXP039498;EXP039523;EXP039534;EXP039536;EXP039537;EXP039544;EXP039547;EXP039548;EXP039550;EXP039569;EXP039572;EXP039574;EXP039576;EXP039579;EXP039581;EXP039586;EXP039609;EXP039610;EXP039612;EXP039618;EXP039620;EXP039638;EXP039639;EXP039645;EXP039663;EXP039667;EXP039677;EXP039678;EXP039695;EXP039696;EXP039699;EXP039700;EXP039704;EXP039705;EXP039707;EXP039723;EXP039740;EXP039745;EXP039759;EXP039762;EXP039772;EXP039781;EXP039782;EXP039802;EXP039805;EXP039806;EXP039832;EXP039840;EXP039845;EXP039854;EXP039861;EXP039864;EXP039866;EXP039880;EXP039898;EXP039929;EXP039943;EXP039949;EXP039952;EXP039954;EXP039968;EXP039971;EXP039980;EXP039983;EXP039994;EXP040000;EXP040012;EXP040026;EXP040028;EXP040031;EXP040050;EXP040054;EXP040061;EXP040063;EXP040068;EXP040071;EXP040072;EXP040077;EXP040082;EXP040087;EXP040089;EXP040101;EXP040107;EXP040118;EXP040128;EXP040134;EXP040145;EXP040169;EXP040192;EXP040193;EXP040228;EXP040241;EXP040244;EXP040246;EXP040253;EXP040254;EXP040256;EXP040257;EXP040275;EXP040279;EXP040282;EXP040290;EXP040291;EXP040298;EXP040321;EXP040383;EXP040384;EXP040385;EXP040425;EXP040426;EXP040768\t\t\tERR173154;ERR173155;ERR173156;ERR173157\t\t\tSRR3405055;SRR3405066;SRR3405078\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for tf in sorted(tfs, key=lambda x: x.gene_name):\n",
    "    Jglobals.write(\"./GRECO.tsv\", tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
